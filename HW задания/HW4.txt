4. Логиcтическая регрессия для K классов (Softmax регрессия)

Реализовать модель логистической регрессии для K классов и обучить на датасете 
digits. Перед обучением произвести стандартизацию векторов характеристик и перевод
меток классов в представление one-hot encoding. Перемешать исходный датасет и 
разделить на обучающую, валидационную выборки. Обучение произвести на обучающей 
выборке методом градиентного спуска, предварительно инициализировав обучаемые 
параметры случайным образом. Во время обучения раз в несколько итераций выводить 
на консоль значения целевой функции и точности (Accuracy) для обучающей и 
валидационной выборок. Отобразить графики зависимости целевой функции от номера 
итерации, точности от номера итерации для обучающей и валидационной выборок. 
Вывести значение точности (Accuracy) модели на валидационной выборке после 
обучения.


Бонусные задания (необязательные):

1. Предусмотреть реализацию в виде класса с отдельными методами на обучение и предсказание
2. Предусмотреть использование регуляризации:
	- weight decay (сумма квадратов обучаемых параметров)
	- dropout
	- labels smoothing
3. Реализовать градиентный спуск на минибатчах (градиент вычисляется только на части выборки 
фиксированного размера).
4. Предусмотреть различные инициализации градиентного спуска (равномерное распределение, 
нормальное, инициализация Xavier, инициализация He)
5. Предусмотреть различные критерии остановки в градиентном спуске
6. Предусмотреть сохранение и считывание модели (например с использованием модуля pickle)
7. Вывести confusion matrix и значения Accuracy для валидационнной выборки до и после обучения
8. Вывести 3 изображения из валидационнной выборки на которых классификатор дал самые уверенные
правильные предсказания и 3 изображения на которых классификатор дал самые уверенные 
неправильные предсказания после обучения.
9. Реализовать отображение графика  целевой функции и графика точности с периодическим 
обновлением во время обучения.
10. Реализовать модель логистической регрессии для K классов с использованием базисных функций.

Детали:
Модель:
	Модель классификатора представлена следующей формулой:

	y = Softmax(Wx + b)

	x - вектор характеристик объекта (размер Dx1)
	W - матрица весов (KxD)
	b - вектор смещений (Kx1)
	y - вектор уверенностей классификатора  (Kx1), принадлежности объекта соответствующему
		 классу

	Softmax функция, определенная следующим образом

	y = Softmax(a)
	y_j = exp(a_j)/Sum(exp(a_i))

	Целевая функция для введенного классификатора определяется через кросс-энтропию:

	E(W,b) = SUM SUM t_ik ln y_k(x_i,W,b)

	t - вектор метки представленный с помощью one-hot encoding (вектор стоящий из нулей 
		кроме одной позиции, где стоит 1. позиция совпадает с номером класса, 
		которому принадлежит вектор характеристик).

	Градиент целевой функции по элементам матрицы W и вектора b (одна из возможных записей 
	(матричный вид))

	nabla_W E = (Y-T).T*X
	nabla_b E = (Y-T).T

Датасет:

	Датасет представляет собой изображения рукописных цифр (1797 экземпляров) и находится в
	библиотеке sklearn. Импорт датасета осуществляется следующим образом
	
	from sklearn.datasets import load_digits

	digits = load_digits()
	digits.data - вектора характеристик (64 элемента), полученные из изображений цифр путем 
		выписывания в одну строку
	digits.images - изображения цифр 8x8
	digits.target - метки изображений, соответствуют цифре на картинке
	digits.target_names - возможные метки, присутствующие в датасете (цифры от 0 до 9)

	Обращение к элементам датасета возможно и по строковым ключам: 
	digits['data'], digits['images'], digits['target'], digits['target_names']


Предобработка данных:

	Перед обучением модели необходимо произвести предобработку данных - сделать 
	стандратизацию. Эта процедура предполагает вычисление вектора средних значений и 
	вектора стандартных отклонений от векторов характеристик:

	mu = 1/N SUM x_i
	sigma = sqrt(1/N SUM (x_i - mu)^2)

	Далее из каждого вектора характеристик вычитается вектор средних значений и полученный
	вектор поэлементно делится на вектор стандартных отклонений. Полученные векторы 
	используют для обучения модели.


Обучение модели (Градиентный спуск):

	Обучение модели (подбор оптимальных значений для W и b) осуществляется с помощью 
	градиентного спуска. 
	Общая схема градиентного спуска выглядит следующим образом:

	w_0 = init()
	while some_criteria_to_stop
		w_{k+1} = w_k - gamma*nabla E(w_k)

	Возможные критерии остановки:
	- количество итераций
	- норма значений между последовательными приближениями меньше определенного значения
	- норма градиента меньше определенного значения
	- значение целевой функции на валидационнной выборке имеем тренд к росту
	- ручная остановка

	Инициализация:
	Возможные инициализации W и b:
	- равномерное распределение на небольшом интервале около нуля U(-epsilon, epsilon)
	- нормальное распределение с нулевым средним и небольшой дисперсией N(0, sigma)
	- Xavier
	- He

Создание класса:

	Пример создания простого класса приведен ниже.

	class Circle:

		def __init__(cx,cy, r):
			self.center_x = cx
			self.center_y = cy
			self.radius = r

		def get_area(self):
			a = np.pi*self.radis**2
			return a

		def get_perimeter(self):
			p = 2*np.pi*self.radius
			return p

	с = Circle(5,5,3)
	print(c.area())
	print(c.perimeter())



Вычисления:

	Для избежания переполнений (overflow, underflow) при вычислении функций softmax и 
	ln воспользоваться следующим:
	1. Для softmax умножить числитель и знаменатель на значение exp(r), где r = - max(a_j)
	2. Для ln раскрыть выражение под логарифмом и воспользовавшись правилами 
	ln(ab) = ln(a) + ln(b), ln(a/b) = ln(a) - ln(b) 
	упростить вычисляемое значение


Сохранение модели:

	Сохранение и считывание модели из бинарного файла с помощью модуля pickle. 
	Сохранение можно производить периодически в течении обучения (например, раз в 
	100 итераций).

	import pickle

	classifier # это обученная модель

	#сохранение модели:

	with open('/path/to/folder/model.pickle', 'wb') as f
		pickle.dump(classifier, f)
		
	#считывание модели
	with open('/path/to/folder/model.pickle', 'rb') as f
		classifier = pickle.load(f)

Использование базисных функций:

	Построить модель логистической регреcсии на К классов с использованием базисных функций.
	Модель принимает следующий вид:

	y = Softmax(W*phi(x) + b)

	где phi(x) - вектор значений базисных функций (размер Mx1). Базисные функции выбрать самим. 
	Необходимо пересчитать градиенты с учетом введенных базисных функций.
	Предполагается, что введение базисных функций потенциально может дать прирост точности.