{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from numpy import ndarray, dot, transpose, ones_like, sum, exp, mean, power, sqrt\n",
    "from numpy.random import seed, randn, permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "def assert_same_shape(array:ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "    \"Two ndarrays should have the same shape:\" \\\n",
    "    \"instead, first ndarray's shae is {0}\" \\\n",
    "    \"and second ndarray's shape is {1}\".format(tuple(array_grad.shape),\n",
    "                                               tuple(array.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "# В Python 3 нет разницы: с object или без него. в Python 2 была, можно оставить для совместимости\n",
    "class Operation(object):\n",
    "    \"\"\"\n",
    "    Базовый класс операции в нейросети\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray):\n",
    "        \"\"\"\n",
    "        Хранение ввода в атрибуте экземпляра self.input_\n",
    "        :param input_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вызов функции self._input_grad().\n",
    "        Проверка совпадения размерностей.\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Метод _output определяется для каждой операции\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise  NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Метод _input_grad определяется для каждой операции\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    \"\"\"\n",
    "    Операция с параметрами\n",
    "    \"\"\"\n",
    "    def __init__(self, param:ndarray):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вызов self._input_grad и self._param_grad\n",
    "        Проверка размерностей\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Во всех подклассах ParamOperation должна быть реализация метода _param_grad\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    \"\"\"\n",
    "    Умножение весов в нейронной сети.\n",
    "    \"\"\"\n",
    "    def __init__(self, W: ndarray):\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление выхода\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return dot(self.input_, self.param)\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление градиента\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return dot(output_grad, transpose(self.param, (1, 0)))\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление градиента параметров\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return dot(transpose(self.input_, (1,0)), output_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "class BiassAdd(ParamOperation):\n",
    "    \"\"\"\n",
    "    Прибавление отклонений\n",
    "    \"\"\"\n",
    "    def __init__(self, B:ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление выхода\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.input_ + self.param\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление градиента параметров\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        param_grad = ones_like(self.param) * output_grad\n",
    "\n",
    "        return sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    \"\"\"\n",
    "    Сигмоидная функция активации\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление выхода\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 1.0/(1.0+exp(-1.0*self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"Вычисление градиента\"\"\"\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    \"\"\"\n",
    "    Уникальная функция активации\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Метод _setup_layer нельзя вызывать прямо в __init__, потмоу что еще нет входных параметров на данном этапе\n",
    "Он будет выпонлятся при тренировке, если self.first == True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    Слой нейронов в нейросети\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons: int):\n",
    "        \"\"\"\n",
    "        :param neurons: Число нейронов (~ ширина слоя)\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.first = True # Первый ли проход через этот слой\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Функция реализуется в каждом слое\n",
    "        :param self:\n",
    "        :param input_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _forward(self, input_:ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Передача входа вперед через серию операций\n",
    "        :param input_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.first:\n",
    "            # Эти настройки производить, если первый проход через слой\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        self._params() # Не было в книге\n",
    "        return self.output\n",
    "\n",
    "    def _backward(self, output_grad: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Передача градиента назад через серию операций\n",
    "        Проверка размерностей\n",
    "        :param output_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "\n",
    "        self._param_grads()\n",
    "        return  input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Извлечение param_grads из операций слоя\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            # является ли ParamOperation подклассом operation.__class__ (класс - подкласс самого себя)\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "        return self.param_grads # Не было в книге\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Извлечение _params из операций слоя\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)\n",
    "        return self.params # Не было в книге"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В _setup_layer тип параметров не совпадает с типом параметров _setup_layer родителя"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Полносвязаный слой, наследующий от Layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()) -> None:\n",
    "        \"\"\"\n",
    "        Для инициализации нужна (нелинейная) функция активации\n",
    "        :param neurons:\n",
    "        :param activation:\n",
    "        \"\"\"\n",
    "        super().__init__(neurons)\n",
    "        # seed_ потом передасться с помощью setattr\n",
    "        self.seed_ = None\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_:ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Определение операций для полносвязного\n",
    "        :param input_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.seed_:\n",
    "            seed(self.seed_)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # веса\n",
    "        self.params.append(randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # отклонения\n",
    "        self.params.append(randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiassAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \"\"\"\n",
    "    Потери нейросети\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Вычисление значения потери\n",
    "        :param prediction:\n",
    "        :param target:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.tarrget = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def _backward(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление градиента потерь по входам функции потерь\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        Функция _output должна реализовываться всем подклассом класса Loss\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Функция должна реализовыватсья всеми подклассами класса Loss\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        \"\"\"\n",
    "        Вычисление среднего квадрата ошибки\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        loss = mean(power(self.prediction - self.tarrget, 2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        \"\"\"\n",
    "        Вычисление градиента ошибки по входу MSE\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 2.0 * (self.prediction - self.tarrget) / self.prediction.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Класс нейронной сети\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed_: float = 1):\n",
    "        \"\"\"\n",
    "        Нейросети нужны слои и поетри\n",
    "        :param layers:\n",
    "        :param loss:\n",
    "        :param seed_:\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed_ = seed_\n",
    "        if seed_:\n",
    "            for layer in self.layers:\n",
    "                # в объекте layer добавить значение атрибуту seed_ (или добавить такой атрибут)\n",
    "                setattr(layer, \"seed_\", self.seed_)\n",
    "\n",
    "    def _forward(self, x_batch: ndarray) -> ndarray:\n",
    "        \"\"\"\n",
    "        Передача данных через последовательность слоев\n",
    "        :param x_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer._forward(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def _backward(self, loss_grad: ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Передача данных назад через последовательность слоев\n",
    "        :param loss_grad:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer._backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Передача данных вперед через последовательность слоев.\n",
    "        Вычисление потерь.\n",
    "        Передача данных назад через последовательность слоев.\n",
    "        :param x_batch:\n",
    "        :param y_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        predictions = self._forward(x_batch)\n",
    "\n",
    "        loss = self.loss._forward(predictions, y_batch)\n",
    "\n",
    "        self._backward(self.loss._backward())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Получение параметров нейросети\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        \"\"\"\n",
    "        Получение градиента потерь по отношению к параметрам нейросети\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # yield добавляет в генератор итераторы\n",
    "            # from из\n",
    "            yield from layer.param_grads\n",
    "            # == for lp in layer.param_grads: yield lp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Базовый класс отмизатора нейросети (Способ обновления весов)\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Обновить веса на основе текущих градиентов\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Стохастический градиентный оптимизатор\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.01):\n",
    "        super().__init__(learning_rate)\n",
    "        # Сеть потом передасться с помощью setattr\n",
    "        self.net = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Для каждого параметра настраивается направление.\n",
    "        Амплитуда регулировки зависит от скорости обучения\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            param -= self.learning_rate * param_grad\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "def permute_data(X, y):\n",
    "    assert X.shape[0] == len(y), \\\n",
    "        \"Размерности x и y не совпдают\"\n",
    "    perm = permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Ответ на вопрос в конце этого файла\n",
    " Не выйдет ли за пределы массива/списка X[ii:ii+size], y[ii:ii+size]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Обучение нейросети\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer):\n",
    "        \"\"\"\n",
    "        Для обучения нужны нейросеть и оптимизатор. Нейросеть\n",
    "        назначается атрибутом экземпляра оптимизатора.\n",
    "        :param net:\n",
    "        :param optim:\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "\n",
    "    def generate_batches(self, X: ndarray, y: ndarray, size: int=32) -> Tuple[ndarray]:\n",
    "        \"\"\"\n",
    "        Генерация партий для тренировки\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        \"Размерности x и y не совпдают\"\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # НЕ БУДЕТ ЛИ ВЫХОДА ЗА ПРЕДЕЛЫ ?\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=10,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed_: int=1,\n",
    "            restart: bool=True) -> None:\n",
    "        \"\"\"\n",
    "        Подгонка нейросети под обучающие данные за некоторое число эпох.\n",
    "        :param X_train:\n",
    "        :param y_train:\n",
    "        :param X_test: На обучение не влияет, нужно для оценки\n",
    "        :param y_test: На обучение не влияет, нужно для оценки\n",
    "        :param epochs:\n",
    "        :param eval_every: Через каждые eval_every эпох выполняется оценка\n",
    "        :param batch_size:\n",
    "        :param seed_:\n",
    "        :param restart: Если True, повторно инициализировать случайно параметры модели при вызове fit\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        seed(seed_)\n",
    "\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "                # first = True => запуск setup в слое => инициализация весов\n",
    "\n",
    "        for e in range(epochs):\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                # Прогон веперд и назад (обновление градиентов)\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                # Обновление весов\n",
    "                self.optim.step()\n",
    "\n",
    "            # Пора ли делать оценку?\n",
    "            if (e+1) % eval_every == 0:\n",
    "                # Аналог predict\n",
    "                test_preds = self.net._forward(X_test)\n",
    "                loss = self.net.loss._forward(test_preds, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "\n",
    "                # Далее по книге реализован откат на шаг к предыдущей оценке (net и optim), если оценка стала хуже. Но для обучения нельзя использвать тестовые данные. Хотя если разделить данные на train, validation и test, то можно использовать validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    \"\"\"\n",
    "    Найти срднюю абсолную ошибку\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return mean(abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    \"\"\"\n",
    "    Найти среднюю квадратическую ошибку\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sqrt(mean(power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    \"\"\"\n",
    "    Найти абсолютную и квадратическую ошибку\n",
    "    :param model:\n",
    "    :param X_test:\n",
    "    :param y_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds = model._forward(X_test).reshape(-1, 1)\n",
    "    print(\"Mean absolute error {:.2f}\".format(mae(preds, y_test)))\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [],
   "source": [
    "linear_regression = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "            activation=Linear())],\n",
    "    loss = MeanSquaredError(),\n",
    "    seed_=20190501\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                 activation=Sigmoid()),\n",
    "            Dense(neurons=1, activation=Linear())],\n",
    "    loss = MeanSquaredError(),\n",
    "    seed_=20190501\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [],
   "source": [
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                  activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                  activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed_=20190501\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray,\n",
    "             type: str=\"col\") -> ndarray:\n",
    "    \"\"\"\n",
    "    1D into 2D\n",
    "    :param a:\n",
    "    :param type:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert a.ndim == 1, \\\n",
    "    \"Размерность должна быть 1\"\n",
    "    if type == \"col\":\n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 30.293\n",
      "Validation loss after 20 epochs is 28.469\n",
      "Validation loss after 30 epochs is 26.293\n",
      "Validation loss after 40 epochs is 25.541\n",
      "Validation loss after 50 epochs is 25.087\n",
      "Mean absolute error 3.52\n",
      "Root mean squared error 5.01\n",
      "________________________\n",
      "Validation loss after 10 epochs is 27.435\n",
      "Validation loss after 20 epochs is 21.839\n",
      "Validation loss after 30 epochs is 18.918\n",
      "Validation loss after 40 epochs is 17.195\n",
      "Validation loss after 50 epochs is 16.215\n",
      "Mean absolute error 2.60\n",
      "Root mean squared error 4.03\n",
      "________________________\n",
      "Validation loss after 10 epochs is 44.143\n",
      "Validation loss after 20 epochs is 25.278\n",
      "Validation loss after 30 epochs is 22.339\n",
      "Validation loss after 40 epochs is 16.500\n",
      "Validation loss after 50 epochs is 14.655\n",
      "Mean absolute error 2.45\n",
      "Root mean squared error 3.83\n",
      "________________________\n"
     ]
    }
   ],
   "source": [
    "list_models = [linear_regression, neural_network, dl]\n",
    "for model in list_models:\n",
    "    optimizer = SGD(learning_rate = 0.01)\n",
    "    trainer = Trainer(model, optimizer)\n",
    "    trainer.fit(X_train, y_train, X_test, y_test,\n",
    "                epochs=50,\n",
    "                eval_every=10,\n",
    "                seed_=20190501)\n",
    "\n",
    "    eval_regression_model(model, X_test, y_test)\n",
    "    print(\"________________________\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(30, 32)\n",
      "[(range(0, 5), range(0, 5)), (range(5, 10), range(5, 10)), (range(10, 15), range(10, 15)), (range(15, 20), range(15, 20)), (range(20, 25), range(20, 25)), (range(25, 30), range(25, 30)), (range(30, 32), range(30, 32))]\n"
     ]
    }
   ],
   "source": [
    "# Тестп \"Почему не выходит за пределы массива\"\n",
    "def gener():\n",
    "    N = 32\n",
    "    X = range(N)\n",
    "    y = range(N)\n",
    "    print(X[30:35])\n",
    "    size = 5\n",
    "    for ii in range(0, N, size):\n",
    "        X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "        yield X_batch, y_batch\n",
    "print([i for i in gener()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}