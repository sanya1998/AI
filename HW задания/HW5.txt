5. Классификатор Random Forest.

Реализовать и обучить классифкатор Random Forest. Обучение и тестирование 
произвести на датасете digits из библиотеки sklearn. Классификатор представить 
в виде множества бинарных деревьев. В качестве критерия качества разделения 
использовать Information Gain (прирост информации), вычисленный на основе 
энтропии. В качестве разделяющих функций использовать разделение гиперплоскостями,
 параллельными осям координат. Обучение отдельных деревьев произвести с помощью 
процедуры random node optimization - определение оптимальных параметров 
разделяющей функции узла на основе значений критерия качества, вычисленного на 
небольшом случайном подмножестве декартова произведения параметров узла. 
Терминальный узел дерева создавать при выполнении хотя бы одного из трех условий:
- энтропия в узле меньше определенного порога
- глубина дерева в текущем узле дистигла максимального значения
- количество элементов обучающей выборки, достигших узла меньше определенного 
порога
Вычислить точность (Accuracy) и построить confusion matrix полученной модели на 
обучающей и тестовой выборках. 


Бонусные задания (необязательные):

1. Вместо энтропии в  критерия качества разделения использовать неопределенность 
Джини (Gini): 
H(S_i) = 1 - SUM (N_i^K/N_i)^2

N_i - множество элементов достигших i узла
N_i^K - множество элементов достигших i узла и принадлежащих классу K

2. Произвести валидацию гиперпараметров с помощью рандомизированной процедуры:
	- количество деревьев
	- максимальная глубина деревьев
	- критерий (энтропия/неопределенности Джини)
	- минимальное значение критерия
	- минимальное количество элементов в терминальных узлах

После валидации вычислить точность и confusion matrix на лучших параметрах.

3. Реализовать вычисление оптимальных параметров разделяющей функции на основе 
полного перебора этих парамтеров. При этом для обеспечения различия в деревьях 
использовать bagging - обучение каждого дерева на случайном подмножестве элементов
обучающей выборки.

4. В качестве разделяющих функций использовать разделение гиперплоскостями не 
параллельными осям координат

5. В качестве разделяющих функций использовать разделение гиперповерхностями, 
полученными с помощью базисных функций phi (нелинейное разделение).